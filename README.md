# Being-M0.5: A Real-time Controllable Vision-Language-Motion Model

<div align="center">

[[Website]](https://beingbeyond.github.io/Being-M0.5)
[[arXiv]](https://arxiv.org/abs/2410.03311)

[![Python Version](https://img.shields.io/badge/Python-3.10-blue.svg)]()
[![GitHub license](https://img.shields.io/badge/MIT-blue)]()

![](images/motion_control.png)
![](images/model_structure.png)
</div>


We present MotionLib, the first million-level dataset for motion generation, which is at least 15Ã— larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named Being-M0, demonstrating robust performance across a wide range of human activities, including unseen ones. More Visualization can be found on our [[Website]](https://beingbeyond.github.io/Being-M0.5).


## Code
We will release our code and part of our dataset soon.

## Citation
If you find our work useful, please consider citing us!
```
@inproceedings{cao2025,
  title={A Real-Time Controllable Vision-Language-Motion Model},
  author={Bin, Cao and Zheng, Sipeng and Ye, Wang and Xia, Lujie and Wei, Qianshan and Jin, Qin and Liu, Jing and Lu, Zongqing},
  booktitle={ICCV},
  year={2025}
}
```